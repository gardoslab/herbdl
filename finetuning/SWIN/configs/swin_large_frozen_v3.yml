# SWIN Large Frozen v3 Configuration
# Model configuration
model:
  model_name_or_path: "microsoft/swin-large-patch4-window7-224-in22k"
  config_name: null
  cache_dir: null
  model_revision: "main"
  image_processor_name: null
  token: null
  trust_remote_code: false
  ignore_mismatched_sizes: true

# Data configuration
data:
  dataset_name: null
  dataset_config_name: null
  data_file: null
  data_dir: null
  train_file: "/projectnb/herbdl/data/kaggle-herbaria/train.json"
  validation_file: "/projectnb/herbdl/data/kaggle-herbaria/val.json"
  image_column_name: "filepath"
  label_column_name: "scientificNameEncoded"
  max_seq_length: 15
  max_train_samples: null
  max_eval_samples: 50000
  overwrite_cache: false
  preprocessing_num_workers: null
  train_val_split: 0.2

# Training configuration
training:
  output_dir: "/projectnb/herbdl/workspaces/faridkar/herbdl/finetuning/output/SWIN/SWIN_LARGE_FROZEN"
  logging_dir: "/projectnb/herbdl/workspaces/faridkar/herbdl/finetuning/output/SWIN/SWIN_LARGE_FROZEN"
  do_train: true
  do_eval: true
  per_device_train_batch_size: 128
  per_device_eval_batch_size: 32
  learning_rate: 0.005
  num_train_epochs: 100
  warmup_steps: 500
  weight_decay: 0.0
  gradient_accumulation_steps: 1
  lr_scheduler_type: "cosine"
  lr_scheduler_kwargs:
    eta_min: 0.000001
  logging_strategy: "epoch"
  save_strategy: "epoch"
  save_total_limit: 5
  eval_strategy: "steps"
  eval_steps: 8964
  report_to: "wandb"
  bf16: true
  dataloader_num_workers: 8
  remove_unused_columns: false
  overwrite_output_dir: false
  seed: 42

# Custom configuration
custom:
  lr_type: "cosine"
  frozen: true
  frozen_type: "v3"  # Freeze all except last 2 transformer blocks and linear layer
  run_group: "SWIN_Large"
  run_name: "SWIN_Large_Frozen_v3"
  run_id: "swin_large_frozen_v3_112425"

# WandB configuration
wandb:
  entity: "gardoslab"
  project: "herbdl"
  resume: "allow"
